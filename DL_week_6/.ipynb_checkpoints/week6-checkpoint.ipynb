{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week5：CNN-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验内容与流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 实验要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1.结合理论课内容，深入理解DenseNet、ResNeXt的结构与FaceNet的工作机制，理解Triplet loss的工作过程。\n",
    "\n",
    " 2.简要回答实验指导书中的提出的问题。按照指导，补充或改写相应代码，使程序能够运行，并得到相应结果。\n",
    "\n",
    " 3.提交作业时，请将实验指导书、pics文件夹和数据集文件夹一同打包提交。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 知识预备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.熟悉基本的分类网络，如AlexNet、VGG、ResNet，GoogleNet。\n",
    "\n",
    "2.理解Residual Block的结构。\n",
    "\n",
    "3.Triplet loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 实验内容 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.在基础的ResNet网络结构之上，增加和修改部分结构，设计基础的DenseNet和ResNeXt网络。\n",
    "\n",
    "2.利用Triplet loss训练一个简单的FaceNet网络，并用knn的方式进行分类预测。\n",
    "\n",
    "3.本次实验用到的数据集是mnist的部分数据，mnist数据集包括50000张训练图片和10000张测试图片，这次选取其中2000张训练图片，每类200张，\n",
    "\n",
    "和1000张测试图片，每类100张，作为本次实验的全部数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DenseNet与ResNeXt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先回顾一下DenseNet的结构，DenseNet的每一层都都与前面层相连，实现了特征重用。\n",
    "\n",
    "下图表示一个DenseBlock\n",
    "![DenseBlock](pics/denseblock.png)\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图所示,在一个DenseBlock中,第i层的输入不仅与i-1层的输出相关,还有所有之前层的输出有关.记作:\n",
    "\n",
    "![output](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 DenseNet网络的搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Growth_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   在一个DenseBlock里面，每个非线性变换H输出的channels数为恒定的Growth_rate，那么第i层的输入的channels数便是k+（i+1）* Growth_rate, k为Input\n",
    "   \n",
    "   的channels数，比如，假设我们把Growth_rate设为4，上图中H1的输入的size为8 * 32 * 32，输出为4 * 32 * 32， 则H2的输入的size为12 * 32 * 32，\n",
    "   \n",
    "   输出还是4 * 32 * 32，H3、H4以此类推，在实验中，用较小的Growth_rate就能实现较好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意， 在一个DenseBlock里面，feature size并没有发生改变，因为需要对不同层的feature map进行concatenate操作，这需要保持相同的feature size。\n",
    "\n",
    "因此在相邻的DenseBlock中间使用Down Sampling来增大感受野，即使用Transition Layer来实现，一般的Transition Layer包含BN、Conv和Avg_pool，\n",
    "\n",
    "同时减少维度，压缩率(compress rate)通常为0.5， 即减少一半的维度。\n",
    "![DenseNet](pics/densenet.png)\n",
    "\n",
    "例如，假设block1的输出c * w * h是24 * 32 * 32，那么经过transition之后，block2的输入就是12 * 16 * 16。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了减少参数和计算量，DenseNet的非线性变换H采用了Bottleneck结构BN-ReLU-Conv(1×1)-BN-ReLU-Conv(3×3)，1×1的卷积用于降低维度，将channels数降\n",
    "\n",
    "低至4 * Growth_rate。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary modules here\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    '''\n",
    "        the above mentioned bottleneck, including two conv layer, one's kernel size is 1×1, another's is 3×3\n",
    "\n",
    "        after non-linear operation, concatenate the input to the output\n",
    "    '''\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        \n",
    "        # input and output are concatenated here\n",
    "        out = torch.cat([out,x], 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    '''\n",
    "        transition layer is used for down sampling the feature\n",
    "        \n",
    "        when compress rate is 0.5, out_planes is a half of in_planes\n",
    "    '''\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv(F.relu(self.bn(x)))\n",
    "        # use average pooling change the size of feature map here\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out \n",
    "\n",
    "    \n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            block: bottleneck\n",
    "            nblock: a list, the elements is number of bottleneck in each denseblock\n",
    "            growth_rate: channel size of bottleneck's output\n",
    "            reduction: \n",
    "        '''\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2*growth_rate\n",
    "        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans1 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "        \n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        # a DenseBlock and a transition layer\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        # the channel size is superposed, mutiply by reduction to cut it down here, the reduction is also known as compress rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "\n",
    "        # only one DenseBlock \n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "\n",
    "        # the last part is a linear layer as a classifier\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(num_planes, num_classes)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        \n",
    "        # number of non-linear transformations in one DenseBlock depends on the parameter you set\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def densenet():\n",
    "    return DenseNet(Bottleneck, [2, 5, 4, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业1\n",
    "上面的定义的DenseNet为多少层DenseNet（只计算卷积层与全连接层）？请定义一个卷积层总数为52层的DenseNet。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet52():\n",
    "    return  # add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def train(epoch, model, lossFunction, optimizer, device, trainloader):\n",
    "    \"\"\"train model using loss_fn and optimizer. When this function is called, model trains for one epoch.\n",
    "    Args:\n",
    "        train_loader: train data\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to judge the distance between target and outputs\n",
    "        optimizer: optimize the loss function\n",
    "        get_grad: True, False\n",
    "    output:\n",
    "        total_loss: loss\n",
    "        average_grad2: average grad for hidden 2 in this epoch\n",
    "        average_grad3: average grad for hidden 3 in this epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    model.train()     # enter train mode\n",
    "    train_loss = 0    # accumulate every batch loss in a epoch\n",
    "    correct = 0       # count when model' prediction is correct i train set\n",
    "    total = 0         # total number of prediction in train set\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # load data to gpu device\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        optimizer.zero_grad()            # clear gradients of all optimized torch.Tensors'\n",
    "        outputs = model(inputs)          # forward propagation return the value of softmax function\n",
    "        loss = lossFunction(outputs, targets) #compute loss\n",
    "        loss.backward()                  # compute gradient of loss over parameters \n",
    "        optimizer.step()                 # update parameters with gradient descent \n",
    "\n",
    "        train_loss += loss.item()        # accumulate every batch loss in a epoch\n",
    "        _, predicted = outputs.max(1)    # make prediction according to the outputs\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            # print loss and acc\n",
    "            print( 'Train loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    print( 'Train loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    \n",
    "    \n",
    "def test(model, lossFunction, optimizer, device, testloader):\n",
    "    \"\"\"\n",
    "    test model's prediction performance on loader.  \n",
    "    When thid function is called, model is evaluated.\n",
    "    Args:\n",
    "        loader: data for evaluation\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to judge the distance between target and outputs\n",
    "    output:\n",
    "        total_loss\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    global best_acc\n",
    "    model.eval() #enter test mode\n",
    "    test_loss = 0 # accumulate every batch loss in a epoch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = lossFunction(outputs, targets) #compute loss\n",
    "\n",
    "            test_loss += loss.item() # accumulate every batch loss in a epoch\n",
    "            _, predicted = outputs.max(1) # make prediction according to the outputs\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item() # count how many predictions is correct\n",
    "        # print loss and acc\n",
    "        print('Test Loss: %.3f  | Test Acc: %.3f%% (%d/%d)'\n",
    "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "        \n",
    "def data_loader():\n",
    "    # define method of preprocessing data for evaluating\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize a tensor image with mean and standard variance\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize a tensor image with mean and standard variance\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    # prepare dataset by ImageFolder, data should be classified by directory\n",
    "    trainset = torchvision.datasets.ImageFolder(root='./mnist/train', transform=transform_train)\n",
    "\n",
    "    testset = torchvision.datasets.ImageFolder(root='./mnist/test', transform=transform_test)\n",
    "\n",
    "    # Data loader. \n",
    "\n",
    "    # Combines a dataset and a sampler, \n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def run(model, num_epochs):\n",
    "    \n",
    "    # load model into GPU device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    if device == 'cuda':\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    # define the loss function and optimizer\n",
    "\n",
    "    lossFunction = nn.CrossEntropyLoss()\n",
    "    lr = 0.01\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    trainloader, testloader = data_loader()\n",
    "    for epoch in range(num_epochs):\n",
    "        train(epoch, model, lossFunction, optimizer, device, trainloader)\n",
    "        test(model, lossFunction, optimizer, device, testloader)\n",
    "        if (epoch + 1) % 50 == 0 :\n",
    "            lr = lr / 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用你自己定义的DenseNet进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training and testing\n",
    "model = densenet52()\n",
    "# num_epochs is adjustable\n",
    "run(model, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 ResNeXt网络的搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cardinality, 指的是repeat layer的个数，下图右边cardinality为32。左图是ResNet的基本结构，输入channel size为64，右图是ResNeXt的基本结构，\n",
    "\n",
    "输入channel size是128，但两者具有相近的参数量。\n",
    "![cardinality](pics/cardinality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ResNeXt Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有三种等价的ResNeXt Block，如下图，a是ResNeXt基本单元，如果把输出那里的1x1合并到一起，得到等价网络b拥有和Inception-ResNet相似的结构，\n",
    "\n",
    "而进一步把输入的1x1也合并到一起，得到等价网络c则和通道分组卷积的网络有相似的结构。\n",
    "![ResNeXtBlock](pics/ResNeXtBlock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ResNeXt网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图表示ResNeXt-50(32x4d)的网络结构，卷积层和全连接层总数为50层，32表示的是cardinality,4d表示每一个repeat layer的channel数为4，所以整个block的通道数是32x4=128.\n",
    "![ResNeXt](pics/ResNeXt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''\n",
    "        Grouped convolution block(c).\n",
    "        \n",
    "    '''\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n",
    "        '''\n",
    "            in_planes: channel size of input\n",
    "            cardinality: number of groups\n",
    "            bottleneck_width: channel size of each group\n",
    "        '''\n",
    "        super(Block, self).__init__()\n",
    "        group_width = cardinality * bottleneck_width\n",
    "        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(group_width)\n",
    "        # divide into 32 groups which 32 is cardinality\n",
    "        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(group_width)\n",
    "        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*group_width:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*group_width)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXt(nn.Module):\n",
    "    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n",
    "        '''\n",
    "            num_blocks: list type, channel size of input\n",
    "            cardinality: number of groups\n",
    "            bottleneck_width: channel size of each group\n",
    "        '''\n",
    "        super(ResNeXt, self).__init__()\n",
    "        self.cardinality = cardinality\n",
    "        self.bottleneck_width = bottleneck_width\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # size 32x32\n",
    "        self.layer1 = self._make_layer(num_blocks[0], 1)\n",
    "        # size 32x32\n",
    "        self.layer2 = self._make_layer(num_blocks[1], 2)\n",
    "        # size 16x16\n",
    "        self.layer3 = self._make_layer(num_blocks[2], 2)\n",
    "        # size 8x8\n",
    "        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n",
    "\n",
    "    def _make_layer(self, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n",
    "            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n",
    "        # Increase bottleneck_width by 2 after each stage.\n",
    "        self.bottleneck_width *= 2\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业2：\n",
    "定义一个ResNeXt-32(16x8d)（补全下面一行代码），仿照DenseNet的方式训练和测试这个网络得出前20个epoch结果，分析这个结果好或坏的原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNeXt32_16x8d =  #TODO， define a required resnext\n",
    "run(ResNeXt32_16x8d, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.FaceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关FaceNet与triplet loss的理论知识请同学们复习理论课有关章节。在这里，我们将用triplet loss训练一个resnet18网络，并用这个网络在mnist数据集上进行KNN分类，具体的，resnet18相当于一个特征提取器，用所有的训练集图片的特征拟合一个KNN分类器，利用这个KNN分类进行预测. 在3.1小节，将给出triplet loss的实现. 3.2小节将实现一个适用于triplet loss训练的resnet18网络. 3.3小节将实现随机选取triplet的dataset, 3.4、3.5小节将分别实现resnet18的训练与测试函数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "facenet的作用是将图像嵌入一个d维的空间，在这个d维空间里，同一类图像的特征之间相隔的近，不同类图像的特征之间相隔的远，这个d我们称之为embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable,Function\n",
    "\n",
    "class PairwiseDistance(Function):\n",
    "    '''\n",
    "        compute distance of the embedding features, p is norm, when p is 2, then return L2-norm distance\n",
    "    '''\n",
    "    def __init__(self, p):\n",
    "        super(PairwiseDistance, self).__init__()\n",
    "        self.norm = p\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        eps = 1e-6  # in case of zeros\n",
    "        diff = torch.abs(x1 - x2)     # subtraction\n",
    "        out = torch.pow(diff, self.norm).sum(dim=1) # square\n",
    "        return torch.pow(out + eps, 1. / self.norm) # L-p norm\n",
    "\n",
    "\n",
    "class TripletLoss(Function):\n",
    "    '''\n",
    "       Triplet loss function.\n",
    "       loss = max(diatance(a,p) - distance(a,n) + margin, 0)\n",
    "       forward method:\n",
    "           args:\n",
    "                anchor, positive, negative\n",
    "           return:\n",
    "                triplet loss\n",
    "    '''\n",
    "    def __init__(self, margin, num_classes=10):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.num_classes = num_classes\n",
    "        self.pdist = PairwiseDistance(2) # to calculate distance\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        d_p = self.pdist.forward(anchor, positive) # distance of anchor and positive\n",
    "        d_n = self.pdist.forward(anchor, negative) # distance of anchor and negative\n",
    "\n",
    "        dist_hinge = torch.clamp(self.margin + d_p - d_n, min=0.0) # ensure loss is no less than zero\n",
    "        loss = torch.mean(dist_hinge)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 resnet-18 for triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    '''\n",
    "        resnet basic block.\n",
    "        one block includes two conv layer and one residual\n",
    "    '''\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class ResNetTriplet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, embedding_size=256, num_classes=10):\n",
    "        super(ResNetTriplet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # feature map size 32x32\n",
    "        self.layer1 = self._make_layer(block, 64,  num_blocks[0], stride=1)\n",
    "        # feature map size 32x32\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        # feature map size 16x16\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        # feature map size 8x8\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # feature map size 4x4\n",
    "        # as we use resnet basic block, the expansion is 1\n",
    "        self.linear = nn.Linear(512*block.expansion, embedding_size)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def l2_norm(self,input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "        output = _output.view(input_size)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        # normalize the features, then we set margin easily\n",
    "        self.features = self.l2_norm(out)\n",
    "        # multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "        alpha = 10\n",
    "        self.features = self.features * alpha\n",
    "        # here we get the 256-d features, next we use those features to make prediction\n",
    "        return self.features\n",
    "\n",
    "    \n",
    "def ResNet18(embedding_size=256, num_classes=10):\n",
    "    \n",
    "    return ResNetTriplet(BasicBlock, [2,2,2,2], embedding_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 triplet dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业3\n",
    "仔细阅读下面代码，对pic_classes的作用进行思考，回答下面问题：下面选取triplet的方式是随机选取，若要改为选择指定类别选取，怎么修改？请写出修改后的两行代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TripletFaceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, csv_name, num_triplets, transform = None):\n",
    "        '''\n",
    "        randomly select triplet,which means anchor,positive and negative are all selected randomly.\n",
    "        args:\n",
    "            root_dir : dir of data set\n",
    "            csv_name : dir of train.csv\n",
    "            num_triplets: total number of triplets\n",
    "        '''\n",
    "        \n",
    "        self.root_dir          = root_dir\n",
    "        self.df                = pd.read_csv(csv_name)\n",
    "        self.num_triplets      = num_triplets\n",
    "        self.transform         = transform\n",
    "        self.training_triplets = self.generate_triplets(self.df, self.num_triplets)\n",
    "    @staticmethod\n",
    "    def generate_triplets(df, num_triplets):\n",
    "        \n",
    "        def make_dictionary_for_pic_class(df):\n",
    "\n",
    "            '''\n",
    "                make csv to the format that we want\n",
    "              - pic_classes = {'class0': [class0_id0, ...], 'class1': [class1_id0, ...], ...}\n",
    "            '''\n",
    "            pic_classes = dict()\n",
    "            for idx, label in enumerate(df['class']):\n",
    "                if label not in pic_classes:\n",
    "                    pic_classes[label] = []\n",
    "                pic_classes[label].append(df.iloc[idx, 0])\n",
    "            return pic_classes\n",
    "        \n",
    "        triplets    = []\n",
    "        classes     = df['class'].unique()\n",
    "        pic_classes = make_dictionary_for_pic_class(df)\n",
    "        \n",
    "        for _ in range(num_triplets):\n",
    "\n",
    "            '''\n",
    "              - randomly choose anchor, positive and negative images for triplet loss\n",
    "              - anchor and positive images in pos_class\n",
    "              - negative image in neg_class\n",
    "              - at least, two images needed for anchor and positive images in pos_class\n",
    "              - negative image should have different class as anchor and positive images by definition\n",
    "            '''\n",
    "        \n",
    "            pos_class = np.random.choice(classes)     # random choose positive class\n",
    "            neg_class = np.random.choice(classes)     # random choose negative class\n",
    "            while len(pic_classes[pos_class]) < 2:\n",
    "                pos_class = np.random.choice(classes)\n",
    "            while pos_class == neg_class:\n",
    "                neg_class = np.random.choice(classes)\n",
    "\n",
    "            pos_name = df.loc[df['class'] == pos_class, 'name'].values[0] # get positive class's name\n",
    "            neg_name = df.loc[df['class'] == neg_class, 'name'].values[0] # get negative class's name\n",
    "\n",
    "            if len(pic_classes[pos_class]) == 2:\n",
    "                ianc, ipos = np.random.choice(2, size = 2, replace = False)\n",
    "            else:\n",
    "                ianc = np.random.randint(0, len(pic_classes[pos_class]))  # random choose anchor\n",
    "                ipos = np.random.randint(0, len(pic_classes[pos_class]))  # random choose positive\n",
    "                while ianc == ipos:\n",
    "                    ipos = np.random.randint(0, len(pic_classes[pos_class]))\n",
    "            ineg = np.random.randint(0, len(pic_classes[neg_class]))      # random choose negative\n",
    "\n",
    "            triplets.append([pic_classes[pos_class][ianc], pic_classes[pos_class][ipos], pic_classes[neg_class][ineg],\n",
    "                 pos_class, neg_class, pos_name, neg_name])\n",
    "        \n",
    "        return triplets\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        anc_id, pos_id, neg_id, pos_class, neg_class, pos_name, neg_name = self.training_triplets[idx]\n",
    "        \n",
    "        anc_img   = os.path.join(self.root_dir, str(pos_name), str(anc_id) + '.png') # join the path of anchor\n",
    "        pos_img   = os.path.join(self.root_dir, str(pos_name), str(pos_id) + '.png') # join the path of positive\n",
    "        neg_img   = os.path.join(self.root_dir, str(neg_name), str(neg_id) + '.png') # join the path of nagetive\n",
    "        \n",
    "        anc_img = Image.open(anc_img).convert('RGB') # open the anchor image\n",
    "        pos_img = Image.open(pos_img).convert('RGB') # open the positive image\n",
    "        neg_img = Image.open(neg_img).convert('RGB') # open the negative image\n",
    "\n",
    "        pos_class = torch.from_numpy(np.array([pos_class]).astype('long'))  # make label transform the type we want\n",
    "        neg_class = torch.from_numpy(np.array([neg_class]).astype('long'))  # make label transform the type we want\n",
    "\n",
    "        data = [anc_img, pos_img,neg_img]\n",
    "        label = [pos_class, pos_class, neg_class]\n",
    "\n",
    "        if self.transform:\n",
    "            data = [self.transform(img)  # preprocessing the image\n",
    "                    for img in data]\n",
    "            \n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.training_triplets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 train function for triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "def train_facenet(epoch, model, optimizer, margin, num_triplets):\n",
    "    model.train()\n",
    "    # preprocessing function for image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.CenterCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "            std=np.array([0.2023, 0.1994, 0.2010])),\n",
    "    ])\n",
    "    \n",
    "    # get dataset of triplet\n",
    "    \n",
    "    # num_triplet is adjustable\n",
    "    train_set = TripletFaceDataset(root_dir     = './mnist/train',\n",
    "                                   csv_name     = './mnist/train.csv',\n",
    "                                   num_triplets = num_triplets,\n",
    "                                   transform    = transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                               batch_size  = 16,\n",
    "                                               shuffle     = True)\n",
    "\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # load data to gpu\n",
    "        data[0], target[0] = data[0].cuda(), target[0].cuda()  # anchor to cuda\n",
    "        data[1], target[1] = data[1].cuda(), target[1].cuda()  # positive to cuda\n",
    "        data[2], target[2] = data[2].cuda(), target[2].cuda()  # negative to cuda\n",
    "\n",
    "        data[0], target[0] = Variable(data[0]), Variable(target[0]) # anchor\n",
    "        data[1], target[1] = Variable(data[1]), Variable(target[1]) # positive\n",
    "        data[2], target[2] = Variable(data[2]), Variable(target[2]) # negative\n",
    "        # zero setting the grad\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        anchor   = model.forward(data[0])\n",
    "        positive = model.forward(data[1])\n",
    "        negative = model.forward(data[2])\n",
    "        \n",
    "        # margin is adjustable\n",
    "        loss = TripletLoss(margin=margin, num_classes=10).forward(anchor, positive, negative) # get triplet loss\n",
    "        total_loss += loss.item()\n",
    "        # back-propagating\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    context = 'Train Epoch: {} [{}/{}], Average loss: {:.4f}'.format(\n",
    "        epoch, len(train_loader.dataset), len(train_loader.dataset), total_loss / len(train_loader))\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 test function for triplet loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于如何测试的问题，由于triplet loss训练的resnet18网络没有分类器，这个网络的最后一层的输出是一个维度为embedding_size的向量，我们把它当作由模型提取出的特征，所以利用这个特征来做测试。首先保存下训练集上所有图片的特征和标签，用sklearn库的KNeighborsClassifier()拟合成一个KNN分类器，这里的K表示领域的个数，K是一个可调节的参数，在测试集上做验证时，提取图片的特征用KNN分类器做预测即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业4：\n",
    "仔细阅读下面代码，回答问题：下面的预测方法为KNN预测，若要改为中心点预测的方式，即找出每个类别的离均值点最近的图片做最近邻预测，请简述找出中心点的方法，无需写代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "def KNN_classifier(model, epoch, n_neighbors):\n",
    "    '''\n",
    "        use all train set data to make KNN classifier\n",
    "    '''\n",
    "    model.eval()\n",
    "    # preprocessing function for image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=np.array([0.485, 0.456, 0.406]),\n",
    "            std=np.array([0.229, 0.224, 0.225])),\n",
    "    ])\n",
    "    # prepare dataset by ImageFolder, data should be classified by directory\n",
    "    train_set = torchvision.datasets.ImageFolder(root='./mnist/train', transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "\n",
    "    features, labels =[], [] # store features and labels\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        #  load data to gpu\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # forward\n",
    "        output = model(data)\n",
    "        # get features and labels to make knn classifier\n",
    "        features.extend(output.data.cpu().numpy())\n",
    "        labels.extend(target.data.cpu().numpy())\n",
    "        \n",
    "    # n_neighbor is adjustable\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(features, labels)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "\n",
    "def test_facenet(epoch, model, clf, test = True):\n",
    "    model.eval()\n",
    "    # preprocessing function for image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=np.array([0.485, 0.456, 0.406]),\n",
    "            std=np.array([0.229, 0.224, 0.225])),\n",
    "    ])\n",
    "    # prepare dataset by ImageFolder, data should be classified by directory\n",
    "    test_set = torchvision.datasets.ImageFolder(root = './mnist/test' if test else './mnist/train', transform = transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size = 32, shuffle = True)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        # load data to gpu\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # forward\n",
    "        output = model.forward(data)\n",
    "        # predict by knn classifier\n",
    "        predicted = clf.predict(output.data.cpu().numpy())\n",
    "        \n",
    "        correct += (torch.tensor(predicted) == target.data.cpu()).sum()\n",
    "        total += target.size(0)\n",
    "\n",
    "    context = 'Accuracy of model in ' + ('test' if test else 'train') + \\\n",
    "              ' set is {}/{}({:.2f}%)'.format(correct, total, 100. * float( correct) / float(total))\n",
    "    print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_facenet():\n",
    "    # hyper parameter\n",
    "    lr = 0.01\n",
    "    margin = 2.0\n",
    "    num_triplets = 1000\n",
    "    n_neighbors = 5\n",
    "    embedding_size = 128\n",
    "    num_epochs=1\n",
    "    \n",
    "    # embedding_size is adjustable\n",
    "    model = ResNet18(embedding_size, 10)\n",
    "    \n",
    "    # load model into GPU device\n",
    "    device = torch.device('cuda:0')\n",
    "    model = model.to(device)\n",
    "    if device == 'cuda':\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        cudnn.benchmark = True\n",
    "    \n",
    "    # define the optimizer, lr、momentum、weight_decay is adjustable\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    print('start training')\n",
    "    for epoch in range(num_epochs):  \n",
    "        train_facenet(epoch, model, optimizer, margin, num_triplets) # train resnet18 with triplet loss\n",
    "        clf = KNN_classifier(model, epoch, n_neighbors)     # get knn classifier\n",
    "        test_facenet(epoch, model, clf, False)  # validate train set\n",
    "        test_facenet(epoch, model, clf, True)   # validate test set\n",
    "        if (epoch + 1) % 4 == 0 :\n",
    "            lr = lr / 3\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_facenet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业5（占20%）：\n",
    "训练一个较好的resnet18网络，收集在测试集上所有预测错误的样本图片（1000张测试集图片，分错不应超过30张，5%）。并在训练集上找出离这个样本最近的同类样本和错类样本的图片，**并作出简要分析**（15%）。例如，对于一个样本sample，正确类别为A，模型将其错分为B，分别找出训练集中A类样本和B类样本中离sample最近的样本图片（注意是图片！**注意一定要保存在pics文件夹或者自定义文件夹一同提交，否则TA看不到，将图片在下面展示出来**）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### hints：重写 test_facenet()函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### hints：根据特征反向寻找图片可参考下列代码. 需保证shuffle=False，train.csv和test.csv均已给出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas\n",
    "frame = pandas.read_csv('./mnist/train.csv')\n",
    "\n",
    "train_set = torchvision.datasets.ImageFolder(root='./mnist/train', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=False)\n",
    "\n",
    "features =[]\n",
    "for i, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    output = model(data)\n",
    "    features.extend(output.data.cpu().numpy())\n",
    "    \n",
    "for index in range(len(features)): \n",
    "    image_path = os.path.join('./mnist', str(frame['name'][index]), str(frame['id'][index]) + '.png')\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Hard triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet loss的性能与采样方式有很大的关系，这里简述两种hard-triplet的采样方式，batch-hard与semi-hard。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每一个minibatch，随机选择P个类，每一类随机挑选K张不同的图片，即一个minibatch有PxK张不同的图片。每一张图片都作为anchor，找出minibatch里面距离anchor最远的正样本和距离最近的负样本，组成一个triplet。loss可表示为：\n",
    "![batch_hard](pics/batch_hard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semi hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与batch-hard不同，semi-hard triplet只需要保证minibatch中anchor到positive的距离小于anchor到negative的距离即为semi-hard，见下图，不需要选出minibatch里面距离anchor最远的负样本\n",
    "![semi_hard](pics/semi_hard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业6：\n",
    "本次实验是分类任务的最后一次实验，你对分类任务的学习有何感想？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业附加题：\n",
    "pytorch实现batch-hard或semi-hard的其中一种，重新训练resnet18，对比上面的随机选择triplet的采样方法，其训练过程和结果有何不同，你有更优的方法吗？（不做不扣分，实现一种有较高加分，鼓励同学们挑战高难度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "# add your batch-hard and semi-hard code here and test them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
